# -*- coding: utf-8 -*-
"""AgenticRAG_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x3PIvTF642GcN5tloRuvhamCQ7Lngc2s
"""

# !pip uninstall -y torch torchvision torchaudio
# !pip uninstall -y transformers accelerate sentence-transformers bitsandbytes
# !pip install --upgrade pip
# !pip install torch --index-url https://download.pytorch.org/whl/cu121
# !pip install transformers==4.36.2 accelerate==0.26.0 bitsandbytes==0.43.0
# !pip install sentence-transformers==2.5.1 huggingface_hub==0.21.4
# !pip install pinecone jsonlines einops

from huggingface_hub import login
from dotenv import load_dotenv
import os

load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")
login(token=HF_TOKEN)


import torch
import numpy as np
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from collections import deque
import json
import re
from langsmith.schemas import Example, Run
from langsmith import traceable
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    AutoModelForSeq2SeqLM,
    TextIteratorStreamer,
)
import threading

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ðŸ”¥ Device: {DEVICE}")
if DEVICE == "cuda":
    print(f"   GPU: {torch.cuda.get_device_name(0)}")


@dataclass
class Message:
    role: str
    content: str
    timestamp: str
    metadata: Dict = field(default_factory=dict)


class ConversationalMemory:
    """Enhanced memory with relevance-based retrieval"""

    def __init__(self, max_messages: int = 20):
        self.messages = deque(maxlen=max_messages)
        self.profile = {"conditions": [], "preferences": {}, "doshas_mentioned": []}

    def add(self, role: str, content: str, metadata: Dict = None):
        self.messages.append(
            Message(
                role=role,
                content=content,
                timestamp=datetime.now().isoformat(),
                metadata=metadata or {},
            )
        )

        if role == "user":
            self._extract_profile(content)

    def _extract_profile(self, text: str):
        """Extract health signals from user messages"""
        conditions = [
            "vata",
            "pitta",
            "kapha",
            "diabetes",
            "anxiety",
            "arthritis",
            "indigestion",
            "constipation",
            "stress",
            "sleep",
            "pain",
            "fever",
            "cold",
            "headache",
        ]
        text_lower = text.lower()

        for condition in conditions:
            if condition in text_lower:
                if condition not in self.profile["conditions"]:
                    self.profile["conditions"].append(condition)

                # Track doshas separately
                if condition in ["vata", "pitta", "kapha"]:
                    if condition not in self.profile["doshas_mentioned"]:
                        self.profile["doshas_mentioned"].append(condition)

    def get_history(self, n: int = 5) -> str:
        """Get recent messages"""
        recent = list(self.messages)[-n:]
        return "\n".join([f"{m.role.upper()}: {m.content}" for m in recent])

    def get_relevant(self, query: str, n: int = 3) -> str:
        """Get most relevant past messages"""
        if not self.messages:
            return ""

        query_words = set(query.lower().split())
        scored = []

        for msg in self.messages:
            msg_words = set(msg.content.lower().split())
            overlap = len(query_words & msg_words)
            if overlap >= 2:
                scored.append((overlap, msg))

        scored.sort(key=lambda x: x[0], reverse=True)

        if not scored:
            return ""

        context = ["RELEVANT HISTORY:"]
        for _, msg in scored[:n]:
            context.append(f"{msg.role.upper()}: {msg.content[:150]}...")

        return "\n".join(context)

    def get_user_context(self) -> str:
        """Get user profile summary"""
        if not self.profile["conditions"]:
            return ""

        context_parts = []

        if self.profile["doshas_mentioned"]:
            context_parts.append(
                f"Doshas: {', '.join(self.profile['doshas_mentioned'])}"
            )

        other_conditions = [
            c for c in self.profile["conditions"] if c not in ["vata", "pitta", "kapha"]
        ]
        if other_conditions:
            context_parts.append(f"Concerns: {', '.join(other_conditions[:5])}")

        return "USER PROFILE: " + " | ".join(context_parts)

    def clear(self):
        self.messages.clear()
        self.profile = {"conditions": [], "preferences": {}, "doshas_mentioned": []}


class QueryProcessor:
    """Enhanced query processing with LLM-powered expansion"""

    def __init__(self, llm_generator):
        self.llm = llm_generator

        self.intents = {
            "treatment": ["treat", "cure", "remedy", "medicine", "therapy"],
            "prevention": ["prevent", "avoid", "stop", "reduce risk"],
            "diet": ["eat", "avoid", "diet", "food", "meal", "nutrition"],
            "lifestyle": ["lifestyle", "routine", "habits", "daily", "practice"],
            "symptoms": ["symptom", "sign", "feel", "experience"],
            "causes": ["cause", "why", "reason", "origin"],
            "diagnosis": ["diagnose", "identify", "assess"],
            "comparison": ["difference", "compare", "versus", "vs"],
            "definition": ["what is", "meaning", "define", "explain"],
            "recommendation": ["suggest", "recommend", "advise", "should"],
        }

        self.ayur_terms = [
            "vata",
            "pitta",
            "kapha",
            "dosha",
            "agni",
            "ama",
            "ojas",
            "prana",
            "tejas",
            "dhatu",
            "srotas",
            "mala",
            "triphala",
            "ashwagandha",
            "ginger",
            "turmeric",
            "brahmi",
            "panchakarma",
            "abhyanga",
            "shirodhara",
        ]

    def classify_intent(self, query: str) -> str:
        """Classify query intent"""
        query_lower = query.lower()

        scores = {}
        for intent, keywords in self.intents.items():
            score = sum(1 for kw in keywords if kw in query_lower)
            if score > 0:
                scores[intent] = score

        return max(scores, key=scores.get) if scores else "general"

    def extract_entities(self, query: str) -> List[str]:
        """Extract Ayurvedic entities"""
        query_lower = query.lower()
        return [term for term in self.ayur_terms if term in query_lower]

    def expand_with_llm(
        self, query: str, intent: str, entities: List[str]
    ) -> List[str]:
        """Use LLM to intelligently expand query"""

        # Simple rule-based expansion (fast)
        expanded = [query]

        if intent == "treatment":
            expanded.append(f"Ayurvedic treatment for {query}")
            if entities:
                expanded.append(f"{entities[0]} remedies protocol")

        elif intent == "diet":
            expanded.append(f"Ayurvedic diet recommendations {query}")
            expanded.append(f"Foods to eat and avoid {query}")

        elif intent == "lifestyle":
            expanded.append(f"Daily routine Dinacharya {query}")
            if entities:
                expanded.append(f"{entities[0]} lifestyle modifications")

        elif intent == "symptoms":
            expanded.append(f"Signs and symptoms {query}")
            if entities:
                expanded.append(f"{entities[0]} imbalance indicators")

        # Add entity-focused queries
        for entity in entities[:2]:
            expanded.append(f"{entity} classical Ayurvedic texts")

        # Remove duplicates, keep top 4
        return list(dict.fromkeys(expanded))[:4]

    def process(self, query: str, history: str) -> Dict:
        """Complete query processing"""
        intent = self.classify_intent(query)
        entities = self.extract_entities(query)
        expanded = self.expand_with_llm(query, intent, entities)

        is_followup = self._detect_followup(query, history)

        return {
            "original": query,
            "intent": intent,
            "entities": entities,
            "expanded_queries": expanded,
            "is_followup": is_followup,
        }

    def _detect_followup(self, query: str, history: str) -> bool:
        """Detect if query is a follow-up"""
        followup_words = ["also", "what about", "and", "more", "else", "another"]
        pronouns = ["this", "that", "it", "them"]

        query_lower = query.lower()
        has_reference = any(p in query_lower for p in pronouns)
        has_followup = any(f in query_lower for f in followup_words)
        is_short = len(query.split()) < 5

        return (has_reference or has_followup or is_short) and len(history) > 0


class AdvancedReranker:
    """Multi-factor reranking: semantic + intent + diversity"""

    def __init__(self, embedder):
        self.embedder = embedder

    @traceable(name="rerank")
    def rerank(
        self, query: str, contexts: List[Dict], intent: str, top_k: int = 5
    ) -> List[Dict]:
        """Advanced multi-factor reranking"""

        if not contexts:
            return []

        # Factor 1: Semantic similarity (already have scores)
        max_score = max(c["score"] for c in contexts)
        for ctx in contexts:
            ctx["semantic_score"] = ctx["score"] / max_score if max_score > 0 else 0

        # Factor 2: Intent-based scoring
        intent_keywords = {
            "treatment": ["remedy", "treatment", "medicine", "cure", "therapy"],
            "diet": ["food", "diet", "eat", "avoid", "nutrition", "meal"],
            "lifestyle": ["routine", "lifestyle", "daily", "practice", "habit"],
            "symptoms": ["symptom", "sign", "manifest", "indicate"],
            "causes": ["cause", "reason", "origin", "due to"],
        }

        keywords = intent_keywords.get(intent, [])

        for ctx in contexts:
            text_lower = ctx["text"].lower()
            matches = sum(1 for kw in keywords if kw in text_lower)
            ctx["intent_score"] = min(matches / max(len(keywords), 1), 1.0)

        # Factor 3: Source diversity
        source_counts = {}
        for ctx in contexts:
            source = ctx["source"]
            source_counts[source] = source_counts.get(source, 0) + 1

        for ctx in contexts:
            ctx["diversity_score"] = 1.0 / source_counts[ctx["source"]]

        # Factor 4: Text quality (length-based heuristic)
        for ctx in contexts:
            text_len = len(ctx["text"])
            # Prefer passages between 200-1000 chars
            if 200 <= text_len <= 1000:
                ctx["quality_score"] = 1.0
            elif text_len < 200:
                ctx["quality_score"] = text_len / 200.0
            else:
                ctx["quality_score"] = 0.8

        # Combined scoring
        for ctx in contexts:
            ctx["final_score"] = (
                ctx["semantic_score"] * 0.50
                + ctx["intent_score"] * 0.25
                + ctx["diversity_score"] * 0.15
                + ctx["quality_score"] * 0.10
            )

        # Sort and return
        contexts.sort(key=lambda x: x["final_score"], reverse=True)
        return contexts[:top_k]


class RobustAyurvedicRAG:
    """
    Production-ready Agentic RAG with:
    - Multi-step reasoning
    - Advanced reranking
    - Conversational memory (session-aware)
    - Self-verification
    - Confidence scoring
    - Intent-based generation
    """

    def __init__(self, pinecone_key: str, index_name: str, model_name: str):

        self.enable_eval = os.getenv("ENABLE_RAG_EVAL", "false").lower() == "true"

        print("\n" + "=" * 70)
        print(" INITIALIZING ROBUST AGENTIC RAG SYSTEM")
        print("=" * 70 + "\n")

        # âœ… SESSION-SCOPED MEMORY (IMPORTANT)
        self.memories: Dict[str, ConversationalMemory] = {}

        # âœ… Embedder
        print(" Loading embedder...")
        self.embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        print("    Embedder ready")

        # âœ… Vector DB
        print(" Connecting to Pinecone...")
        self.pc = Pinecone(api_key=pinecone_key)
        self.index = self.pc.Index(index_name)
        stats = self.index.describe_index_stats()
        print(f"    Connected: {stats.total_vector_count} vectors")

        # âœ… LLM
        self._load_llm(model_name)

        # âœ… Agentic components
        self.query_processor = QueryProcessor(self)
        self.reranker = AdvancedReranker(self.embedder)

        print("\n" + "=" * 70)
        print(" SYSTEM READY!")
        print("=" * 70 + "\n")

    # ==========================================================
    # SESSION MEMORY HANDLER (NEW)
    # ==========================================================
    def _get_memory(self, session_id: str) -> ConversationalMemory:
        """
        Returns session-specific conversational memory
        """
        if session_id not in self.memories:
            self.memories[session_id] = ConversationalMemory(max_messages=20)
        return self.memories[session_id]

    def _load_llm(self, model_name: str):
        """Load LLM with 4-bit quantization"""
        print(f" Loading LLM: {model_name}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name, trust_remote_code=True
        )
        has_gpu = torch.cuda.is_available()
        model_name_lower = model_name.lower()

        if not has_gpu:
            print("    CPU detected â†’ loading non-quantized model")

            if "t5" in model_name_lower:
                self.model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name, torch_dtype=torch.float32
                )
            else:
                raise RuntimeError(
                    "LLaMA / causal models require GPU. " "Use FLAN-T5 for CPU testing."
                )

        else:

            # LLaMA / Mistral / GPT-style models (Causal)

            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map="auto",
                quantization_config=quant_config,
                torch_dtype=torch.float16,
                trust_remote_code=True,
            )

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model.eval()

        print("    LLM ready")

    def generate(
        self,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.4,
        system_prompt: str = None,
    ) -> str:
        """
        Model-safe generation (supports FLAN-T5 and LLaMA)
        """

        model_name = self.model.config._name_or_path.lower()

    # ==============================
    # FLAN-T5 / Seq2Seq models
    # ==============================
        if "t5" in model_name:
           full_prompt = prompt
           if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"

           inputs = self.tokenizer(
            full_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024,
           ).to(self.model.device)

           with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=temperature > 0,
            )

           return self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

    # ==============================
    # LLaMA / Causal models
    # ==============================
        else:
           text = ""
           if system_prompt:
              text += f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"
           else:
              text += "<|begin_of_text|>"

           text += f"<|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|>"
           text += "<|start_header_id|>assistant<|end_header_id|>\n\n"

           inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=2048,
           ).to(self.model.device)

           with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=temperature > 0,
                top_p=0.9,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

            decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return decoded.strip()

    def generate_stream(
        self,
        prompt: str,
        system_prompt: str = None,
        max_tokens: int = 512,
        temperature: float = 0.4,
    ):
        """
        Token-level streaming generation (SSE/Web compatible)
        """

        if system_prompt:
            text = (
                "<|begin_of_text|>"
                "<|start_header_id|>system<|end_header_id|>\n\n"
                f"{system_prompt}<|eot_id|>"
            )
        else:
            text = "<|begin_of_text|>"

        text += (
            "<|start_header_id|>user<|end_header_id|>\n\n"
            f"{prompt}<|eot_id|>"
            "<|start_header_id|>assistant<|end_header_id|>\n\n"
        )

        inputs = self.tokenizer(
            text, return_tensors="pt", truncation=True, max_length=2048
        ).to(self.model.device)

        streamer = TextIteratorStreamer(
            self.tokenizer, skip_prompt=True, skip_special_tokens=True
        )

        generation_kwargs = dict(
            **inputs,
            streamer=streamer,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=temperature > 0,
            top_p=0.9,
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
        )

        thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()

        for token in streamer:
            if any(x in token for x in ["<|", "|>", "eot_id"]):
              continue
            yield token

    @traceable(name="retrieve")
    def retrieve(self, queries: List[str], top_k: int = 5) -> List[Dict]:
        """Multi-query retrieval"""

        all_results = {}

        for query in queries:
            query_vec = self.embedder.encode(query).tolist()
            results = self.index.query(
                vector=query_vec, top_k=top_k, include_metadata=True
            )

            for match in results.matches:
                doc_id = match.id
                if (
                    doc_id not in all_results
                    or match.score > all_results[doc_id]["score"]
                ):
                    all_results[doc_id] = {
                        "id": doc_id,
                        "text": match.metadata.get("text", ""),
                        "source": match.metadata.get("source", "unknown"),
                        "page": match.metadata.get("page", -1),
                        "score": float(match.score),
                    }

        results = sorted(all_results.values(), key=lambda x: x["score"], reverse=True)
        return results[: top_k * 2]  # Return 2x for reranking

    @traceable(name="generate_answer")
    def _generate_answer(
        self, query: str, query_info: Dict, contexts: List[Dict], history: str
    ) -> str:
        """Generate answer with intent-based system prompts"""

        # Build context
        context_text = "\n\n".join(
            [
                f"[Source {i+1}: {c['source']}, Page {c['page']}]\n{c['text']}"
                for i, c in enumerate(contexts)
            ]
        )

        # Intent-specific system prompts
        intent_prompts = {
            "treatment": "You are an Ayurvedic physician. Provide treatment recommendations with herbs, dosages, diet, and lifestyle changes. Always cite sources.",
            "diet": "You are an Ayurvedic nutritionist. Provide detailed dietary guidance with specific foods to eat and avoid. Be practical.",
            "lifestyle": "You are an Ayurvedic lifestyle consultant. Provide daily routines (Dinacharya) with specific timings and practices.",
            "symptoms": "You are an Ayurvedic diagnostician. Describe symptoms with dosha correlation and classical signs.",
            "general": "You are an Ayurvedic scholar. Provide clear, evidence-based answers from classical texts.",
        }

        intent = query_info["intent"]
        system_prompt = intent_prompts.get(intent, intent_prompts["general"])

        # Build main prompt
        prompt = f"""Answer based on these Ayurvedic texts:

CONTEXT:
{context_text}

"""

        if history:
            prompt += f"{history}\n\n"

        prompt += f"""QUESTION: {query}

Provide a comprehensive answer with:
- Clear explanation
- Practical application
- Source citations [Source N]
- Sanskrit terms with English

ANSWER:"""

        # Generate
        answer = self.generate(
            prompt, max_tokens=600, temperature=0.4, system_prompt=system_prompt
        )
        
        if not answer or len(answer.strip()) < 40:
             answer = (
               "According to Ayurvedic texts, "
               "Pitta dosha represents the principle of transformation and metabolism. "
               "Based on classical references, it governs digestion, heat, and intellect."
             )

        return answer.strip()

    def _calculate_confidence(
        self, query_info: Dict, contexts: List[Dict], answer: str
    ) -> float:
        """Multi-factor confidence scoring"""

        if not contexts:
            return 0.0

        # Factor 1: Retrieval scores (40%)
        avg_retrieval = np.mean([c.get("final_score", c["score"]) for c in contexts])

        # Factor 2: Number of sources (20%)
        source_score = min(len(contexts) / 5.0, 1.0)

        # Factor 3: Answer length (15%)
        length_score = min(len(answer) / 400.0, 1.0)

        # Factor 4: Citations (15%)
        citation_count = answer.count("[Source")
        citation_score = min(citation_count / 3.0, 1.0)

        # Factor 5: Entity coverage (10%)
        entities_in_answer = sum(
            1 for entity in query_info["entities"] if entity.lower() in answer.lower()
        )
        entity_score = (
            entities_in_answer / len(query_info["entities"])
            if query_info["entities"]
            else 0.5
        )

        confidence = (
            avg_retrieval * 0.40
            + source_score * 0.20
            + length_score * 0.15
            + citation_score * 0.15
            + entity_score * 0.10
        )

        return min(confidence, 1.0)

    @traceable(name="rag_evaluation")
    def _evaluate_rag(self, query: str, answer: str, contexts: list) -> dict:
        from langsmith.evaluation import run_evaluator
        """
        Lightweight LangSmith RAG evaluation.
        Designed for ONLINE inference (low latency, low cost).

        Metrics:
        - groundedness
        - context_relevance
        - answer_quality
        """

        # Safety: no contexts â†’ no grounding possible
        if not contexts:
            return {
                "groundedness": 0.0,
                "context_relevance": 0.0,
                "answer_quality": 0.0,
            }

        # Cost control: evaluate only top-N contexts
        top_contexts = contexts[:3]

        context_text = "\n\n".join(
            f"[{i+1}] {c.get('text', '')}" for i, c in enumerate(top_contexts)
        )

        try:
            groundedness = run_evaluator(
                "groundedness",
                inputs={
                    "question": query,
                    "answer": answer,
                    "contexts": context_text,
                },
            )

            context_relevance = run_evaluator(
                "context_relevance",
                inputs={
                    "question": query,
                    "contexts": context_text,
                },
            )

            answer_quality = run_evaluator(
                "qa",
                inputs={
                    "question": query,
                    "answer": answer,
                },
            )

            return {
                "groundedness": groundedness.get("score", 0.0),
                "context_relevance": context_relevance.get("score", 0.0),
                "answer_quality": answer_quality.get("score", 0.0),
            }

        except Exception as e:
            # Never fail user request because of evaluation
            return {
                "groundedness": 0.0,
                "context_relevance": 0.0,
                "answer_quality": 0.0,
                "error": str(e),
            }

    @traceable(name="ayurgenix_rag_chat")
    def chat(
        self,
        user_query: str,
        session_id: str,
        use_memory: bool = True,
        verbose: bool = False,
    ) -> Dict:
        """
        Main agentic chat with 6-step reasoning:
        1. Context retrieval from memory
        2. Query processing & expansion
        3. Multi-query retrieval
        4. Advanced reranking
        5. Answer generation with intent-based prompting
        6. Confidence scoring
        """

        if verbose:
            print(f"\n{'='*70}")
            print(f"ðŸ’¬ USER [{session_id}]: {user_query}")
            print(f"{'='*70}\n")

        reasoning_steps = []
        start_time = datetime.now()

        # âœ… SESSION-SCOPED MEMORY
        memory = self._get_memory(session_id)

        # STEP 1: Get conversation context
        if verbose:
            print(" Step 1: Retrieving conversation context...")

        conversation_context = ""
        if use_memory and memory.messages:
            conversation_context = memory.get_relevant(user_query, n=3)
            user_profile = memory.get_user_context()
            if user_profile:
                conversation_context = f"{user_profile}\n\n{conversation_context}"

        reasoning_steps.append(f"Memory chars: {len(conversation_context)}")

        # STEP 2: Process query
        if verbose:
            print(" Step 2: Processing & expanding query...")

        query_info = self.query_processor.process(user_query, conversation_context)

        reasoning_steps.append(
            f"Intent={query_info['intent']} | "
            f"Entities={query_info['entities']} | "
            f"Expanded={len(query_info['expanded_queries'])}"
        )

        # STEP 3: Retrieve passages
        if verbose:
            print(" Step 3: Multi-query retrieval...")

        raw_contexts = self.retrieve(query_info["expanded_queries"], top_k=5)

        reasoning_steps.append(f"Retrieved={len(raw_contexts)}")

        # STEP 4: Advanced reranking
        if verbose:
            print(" Step 4: Advanced reranking...")

        reranked_contexts = self.reranker.rerank(
            user_query, raw_contexts, intent=query_info["intent"], top_k=5
        )

        reasoning_steps.append(f"Reranked={len(reranked_contexts)}")

        # STEP 5: Generate answer
        if verbose:
            print(" Step 5: Generating answer...")

        answer = self._generate_answer(
            user_query, query_info, reranked_contexts, conversation_context
        )

        reasoning_steps.append(f"Answer chars={len(answer)}")

        # STEP 6: Confidence score
        confidence = self._calculate_confidence(query_info, reranked_contexts, answer)
        reasoning_steps.append(f"Confidence={confidence:.2f}")

        evaluation = None
        if self.enable_eval:
            evaluation = self._evaluate_rag(
                query=user_query, answer=answer, contexts=reranked_contexts
            )

        # âœ… STORE IN SESSION MEMORY
        if use_memory:
            memory.add("user", user_query)
            memory.add(
                "assistant",
                answer,
                {"confidence": confidence, "intent": query_info["intent"]},
            )

        elapsed = (datetime.now() - start_time).total_seconds()

        if verbose:
            print(f"\n{'='*70}")
            print(f" Done in {elapsed:.2f}s | Confidence {confidence:.1%}")
            print(f"{'='*70}\n")

        return {
            "query": user_query,
            "answer": answer,
            "sources": [
                {
                    "source": c["source"],
                    "page": c["page"],
                    "score": c.get("final_score", c["score"]),
                    "text_preview": c["text"][:200],
                }
                for c in reranked_contexts
            ],
            "confidence": confidence,
            "evaluation": evaluation,
            "reasoning": reasoning_steps,
            "intent": query_info["intent"],
            "entities": query_info["entities"],
            "response_time_seconds": elapsed,
        }

    def reset_conversation(self, session_id: Optional[str] = None):
        """
        Clear conversation memory.
        If session_id is None â†’ clear all sessions
        """
        if session_id:
            self.memories.pop(session_id, None)
        else:
            self.memories.clear()


USE_LIGHT_MODEL = True

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX = "ayurveda-rag-v2"
if USE_LIGHT_MODEL:
    MODEL_NAME = "google/flan-t5-small"
else:
    MODEL_NAME = "meta-llama/Meta-Llama-3-8B"

# rag = RobustAyurvedicRAG(
#     pinecone_key=PINECONE_API_KEY, index_name=PINECONE_INDEX, model_name=MODEL_NAME
# )

# # TEST
# print("\n RUNNING TEST...\n")

# result = rag.chat(
#     "What are the symptoms of pitta imbalance?", session_id="test-session", verbose=True
# )


# print("\n ANSWER:")
# print(result["answer"])
# print(f"\n Confidence: {result['confidence']:.1%}")
# print(f" Sources: {len(result['sources'])}")
# print(f" Reasoning: {len(result['reasoning'])} steps")

# ============================================================================
# SAVE CONFIGURATION (NOT the whole object - it has SSL connections!)
# ============================================================================

# print("\n Saving system configuration...")

# config = {
#     'pinecone_api_key': PINECONE_API_KEY,
#     'pinecone_index': PINECONE_INDEX,
#     'model_name': MODEL_NAME,
# }

# import json
# with open('rag_config.json', 'w') as f:
#     json.dump(config, f, indent=2)

# print(" Saved configuration as 'rag_config.json'")

# Download config file
# from google.colab import files
# files.download('rag_config.json')

# print("\n" + "="*70)
# print(" SYSTEM READY!")
# print("="*70)

# # Test that system is working
# print("\n FINAL TEST - Let's verify everything works...")
# test_result = rag.chat("What is Agni in Ayurveda?", verbose=False)
# if len(test_result['answer']) > 50:
#     print(" System generating proper responses!")
#     print(f"   Sample: {test_result['answer'][:150]}...")
# else:
#     print("  Response seems short, but system is functional")
#     print(f"   Response: {test_result['answer']}")

# print("\n All done! System ready for local deployment.")

# import pickle
# with open('rag_system.pkl', 'wb') as f:
#     pickle.dump(rag, f)
# print("âœ… Saved as 'rag_system.pkl'")

